{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc4ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pl-bert/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/pl-bert/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import LoggerType\n",
    "\n",
    "from transformers import AdamW, AlbertConfig, AlbertModel, AutoTokenizer\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "from model import MultiTaskModel\n",
    "from dataloader import build_dataloader\n",
    "from utils import length_to_mask, scan_checkpoint\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d0c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pickle\n",
    "\n",
    "config_path = \"Configs/config.yml\"\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f31226d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['dataset_params']['tokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e60819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # F0 loss (regression)\n",
    "\n",
    "best_loss = float('inf')  # best test loss\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "loss_train_record = list([])\n",
    "loss_test_record = list([])\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "log_interval = config['log_interval']\n",
    "save_interval = config['save_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4fa9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "\n",
    "    curr_steps = 0\n",
    "\n",
    "    dataset = load_dataset(config[\"data_folder\"], split=\"train\")\n",
    "\n",
    "    log_dir = config[\"log_dir\"]\n",
    "    if not osp.exists(log_dir):\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "    shutil.copy(config_path, osp.join(log_dir, osp.basename(config_path)))\n",
    "\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    train_loader = build_dataloader(\n",
    "        dataset, batch_size=batch_size, num_workers=0, dataset_config=config[\"dataset_params\"]\n",
    "    )\n",
    "\n",
    "    albert_base_configuration = AlbertConfig(**config[\"model_params\"])\n",
    "\n",
    "    bert = AlbertModel(albert_base_configuration)\n",
    "    bert = MultiTaskModel(\n",
    "        bert,\n",
    "        num_vocab=len(tokenizer),\n",
    "        num_tokens=config[\"model_params\"][\"vocab_size\"],\n",
    "        hidden_size=config[\"model_params\"][\"hidden_size\"],\n",
    "    )\n",
    "\n",
    "    load = True\n",
    "    try:\n",
    "        files = os.listdir(log_dir)\n",
    "        ckpts = []\n",
    "        for f in os.listdir(log_dir):\n",
    "            if f.startswith(\"step_\"):\n",
    "                ckpts.append(f)\n",
    "\n",
    "        iters = [int(f.split(\"_\")[-1].split(\".\")[0]) for f in ckpts if os.path.isfile(os.path.join(log_dir, f))]\n",
    "        iters = sorted(iters)[-1]\n",
    "    except:\n",
    "        iters = 0\n",
    "        load = False\n",
    "\n",
    "    optimizer = AdamW(bert.parameters(), lr=1e-4)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config[\"mixed_precision\"], split_batches=True, kwargs_handlers=[ddp_kwargs]\n",
    "    )\n",
    "\n",
    "    if load:\n",
    "        checkpoint = torch.load(log_dir + \"/step_\" + str(iters) + \".t7\", map_location=\"cpu\")\n",
    "        state_dict = checkpoint[\"net\"]\n",
    "        from collections import OrderedDict\n",
    "\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:]  # remove `module.`\n",
    "            new_state_dict[name] = v\n",
    "\n",
    "        bert.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "        accelerator.print(\"Checkpoint loaded.\")\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    bert, optimizer, train_loader = accelerator.prepare(bert, optimizer, train_loader)\n",
    "\n",
    "    accelerator.print(\"Start training...\")\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(train_loader):\n",
    "        curr_steps += 1\n",
    "\n",
    "        words, labels, phonemes, input_lengths, masked_indices = batch\n",
    "        text_mask = length_to_mask(torch.Tensor(input_lengths)).to(accelerator.device)\n",
    "\n",
    "        tokens_pred, words_pred = bert(phonemes, attention_mask=(~text_mask).int())\n",
    "\n",
    "        loss_vocab = 0\n",
    "        for _s2s_pred, _text_input, _text_length, _masked_indices in zip(\n",
    "            words_pred, words, input_lengths, masked_indices\n",
    "        ):\n",
    "            loss_vocab += criterion(_s2s_pred[:_text_length], _text_input[:_text_length])\n",
    "        loss_vocab /= words.size(0)\n",
    "\n",
    "        loss_token = 0\n",
    "        sizes = 1\n",
    "        for _s2s_pred, _text_input, _text_length, _masked_indices in zip(\n",
    "            tokens_pred, labels, input_lengths, masked_indices\n",
    "        ):\n",
    "            if len(_masked_indices) > 0:\n",
    "                _text_input = _text_input[:_text_length][_masked_indices]\n",
    "                loss_tmp = criterion(_s2s_pred[:_text_length][_masked_indices], _text_input[:_text_length])\n",
    "                loss_token += loss_tmp\n",
    "                sizes += 1\n",
    "        loss_token /= sizes\n",
    "\n",
    "        loss = loss_vocab + loss_token\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        iters = iters + 1\n",
    "        if (iters + 1) % log_interval == 0:\n",
    "            accelerator.print(\n",
    "                \"Step [%d/%d], Loss: %.5f, Vocab Loss: %.5f, Token Loss: %.5f\"\n",
    "                % (iters + 1, num_steps, running_loss / log_interval, loss_vocab, loss_token)\n",
    "            )\n",
    "            running_loss = 0\n",
    "\n",
    "        if (iters + 1) % save_interval == 0:\n",
    "            accelerator.print(\"Saving..\")\n",
    "\n",
    "            state = {\n",
    "                \"net\": bert.state_dict(),\n",
    "                \"step\": iters,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "\n",
    "            accelerator.save(state, log_dir + \"/step_\" + str(iters + 1) + \".t7\")\n",
    "\n",
    "        if curr_steps > num_steps:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c4b8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 493/493 [00:00<00:00, 910kB/s]\n",
      "Downloading data: 100%|██████████| 17/17 [00:37<00:00,  2.19s/files]\n",
      "Generating train split: 100%|██████████| 9839572/9839572 [01:51<00:00, 88531.57 examples/s] \n",
      "/root/miniconda3/envs/pl-bert/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/pl-bert/lib/python3.8/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Start training...\n",
      "Step [10/1000000], Loss: 10.99973, Vocab Loss: 8.78823, Token Loss: 2.81447\n",
      "Step [20/1000000], Loss: 11.06915, Vocab Loss: 8.04384, Token Loss: 2.95215\n",
      "Step [30/1000000], Loss: 10.77548, Vocab Loss: 8.36543, Token Loss: 2.69089\n",
      "Step [40/1000000], Loss: 10.41517, Vocab Loss: 7.35178, Token Loss: 2.79306\n",
      "Step [50/1000000], Loss: 10.11564, Vocab Loss: 7.03958, Token Loss: 2.87688\n",
      "Step [60/1000000], Loss: 10.05284, Vocab Loss: 6.80095, Token Loss: 2.66740\n",
      "Step [70/1000000], Loss: 9.49882, Vocab Loss: 6.52335, Token Loss: 3.01062\n",
      "Step [80/1000000], Loss: 9.43032, Vocab Loss: 5.90150, Token Loss: 2.67500\n",
      "Step [90/1000000], Loss: 9.88101, Vocab Loss: 7.33918, Token Loss: 3.19278\n",
      "Step [100/1000000], Loss: 9.49276, Vocab Loss: 6.62324, Token Loss: 2.64060\n",
      "Step [110/1000000], Loss: 9.30281, Vocab Loss: 6.59688, Token Loss: 2.79078\n",
      "Step [120/1000000], Loss: 9.36378, Vocab Loss: 6.84569, Token Loss: 2.69487\n",
      "Step [130/1000000], Loss: 9.66182, Vocab Loss: 7.56745, Token Loss: 3.14256\n",
      "Step [140/1000000], Loss: 9.40929, Vocab Loss: 6.41638, Token Loss: 2.75937\n",
      "Step [150/1000000], Loss: 9.45642, Vocab Loss: 7.41654, Token Loss: 2.80198\n",
      "Step [160/1000000], Loss: 9.14693, Vocab Loss: 6.54291, Token Loss: 2.72436\n",
      "Step [170/1000000], Loss: 9.29787, Vocab Loss: 7.11756, Token Loss: 2.80106\n",
      "Step [180/1000000], Loss: 8.95515, Vocab Loss: 6.65159, Token Loss: 2.66409\n",
      "Step [190/1000000], Loss: 9.49005, Vocab Loss: 6.68375, Token Loss: 2.92245\n",
      "Step [200/1000000], Loss: 9.04662, Vocab Loss: 6.88965, Token Loss: 2.93872\n",
      "Step [210/1000000], Loss: 9.29771, Vocab Loss: 6.37243, Token Loss: 2.89018\n",
      "Step [220/1000000], Loss: 9.06198, Vocab Loss: 5.85421, Token Loss: 2.74560\n",
      "Step [230/1000000], Loss: 9.12330, Vocab Loss: 6.36298, Token Loss: 2.87789\n",
      "Step [240/1000000], Loss: 8.97523, Vocab Loss: 5.99993, Token Loss: 2.75081\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "while True:\n",
    "    notebook_launcher(train, args=(), num_processes=1, use_port=33389)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53ad7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
